{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Pandas is a high-performance library for doing data analysis in Python... if you use it correctly. Today we'll go through some common performance traps people fall into, and we'll see how to stay on the fast path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Measure twice, cut once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "We know the dangers of [premature optimization](https://en.wikipedia.org/wiki/Program_optimization#When_to_optimize), so before you spend time speeding up some code, it's worth verifying that the code in question is *actually* slow, and identify exactly where it's slow. For this, I prefer tools like\n",
    "\n",
    "* [snakeviz](https://jiffyclub.github.io/snakeviz/) for function-level profiling\n",
    "* [line-profiler](https://github.com/pyutils/line_profiler) meauring specific functions line-by-line.\n",
    "\n",
    "See https://tomaugspurger.github.io/maintaing-performance.html for more on how to use these tools to identify slow sections of code. From here no out, we'll assume you've already verified that some code needs optimizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Storage formats and I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "Your first interaction with pandas is often one of the `pd.read_<format>` functions. While pandas supports reading from many different formats, some are higher-performance than others. In particular, we'll compare two specific formats\n",
    "\n",
    "* CSV\n",
    "* Parquet\n",
    "\n",
    "If your workload is IO-bound and if you're lucky enough to choose your storage format, switching your storage format can have a big speedup. Let's generate some data for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load utils.py\n",
    "# A helper to generate some dummy data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "names = [\"Alice\", \"Bob\", \"Charlie\", \"Dan\", \"Edith\", \"Frank\", \"George\", \"Hannah\", \"Ingrid\", \"Jerry\", \"Kevin\", \"Laura\", \"Michael\", \"Norbert\", \"Oliver\", \"Patricia\", \"Quinn\", \"Ray\", \"Sarah\", \"Tim\", \"Ursula\", \"Victor\", \"Wendy\", \"Xavier\", \"Yvonne\", \"Zelda\",]\n",
    "\n",
    "\n",
    "def make_float(n, rstate):\n",
    "    return rstate.rand(n) * 2 - 1\n",
    "\n",
    "\n",
    "def make_int(n, rstate, lam=1000):\n",
    "    return rstate.poisson(lam, size=n)\n",
    "\n",
    "def make_string(n, rstate):\n",
    "    return rstate.choice(names, size=n)\n",
    "\n",
    "\n",
    "def make_categorical(n, rstate):\n",
    "    return pd.Categorical.from_codes(rstate.randint(0, len(names), size=n), names)\n",
    "\n",
    "\n",
    "make = {\n",
    "    float: make_float,\n",
    "    int: make_int,\n",
    "    str: make_string,\n",
    "    object: make_string,\n",
    "    \"category\": make_categorical,\n",
    "}\n",
    "\n",
    "\n",
    "def make_timeseries_part(\n",
    "        start=\"2000-01-01\",\n",
    "        end=\"2000-01-31\",\n",
    "        dtypes={\"name\": \"category\", \"id\": int, \"x\": float, \"y\": float},\n",
    "        freq=\"10s\",\n",
    "        random_state=None,\n",
    "        kwargs=None\n",
    "    ):\n",
    "    kwargs = kwargs or {}\n",
    "    index = pd.date_range(start=start, end=end, freq=freq, name=\"timestamp\")\n",
    "    state = np.random.RandomState(random_state)\n",
    "    columns = {}\n",
    "    for k, dt in dtypes.items():\n",
    "        kws = {\n",
    "            kk.rsplit(\"_\", 1)[1]: v\n",
    "            for kk, v in kwargs.items()\n",
    "            if kk.rsplit(\"_\", 1)[0] == k\n",
    "        }\n",
    "        columns[k] = make[dt](len(index), state, **kws)\n",
    "    df = pd.DataFrame(columns, index=index, columns=sorted(columns))\n",
    "    if df.index[-1] == end:\n",
    "        df = df.iloc[:-1]\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_timeseries(\n",
    "    start=\"2000-01-01\",\n",
    "    end=\"2000-12-31\",\n",
    "    dtypes={\"name\": str, \"id\": int, \"x\": float, \"y\": float},\n",
    "    freq=\"10s\",\n",
    "    partition_freq=\"1M\",\n",
    "    seed=None,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"Create timeseries dataframe with random data\n",
    "    Parameters\n",
    "    ----------\n",
    "    start: datetime (or datetime-like string)\n",
    "        Start of time series\n",
    "    end: datetime (or datetime-like string)\n",
    "        End of time series\n",
    "    dtypes: dict\n",
    "        Mapping of column names to types.\n",
    "        Valid types include {float, int, str, 'category'}\n",
    "    freq: string\n",
    "        String like '2s' or '1H' or '12W' for the time series frequency\n",
    "    partition_freq: string\n",
    "        String like '1M' or '2Y' to divide the dataframe into partitions\n",
    "    seed: int (optional)\n",
    "        Randomstate seed\n",
    "    kwargs:\n",
    "        Keywords to pass down to individual column creation functions.\n",
    "        Keywords should be prefixed by the column name and then an underscore.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import dask.dataframe as dd\n",
    "    >>> df = dd.demo.make_timeseries('2000', '2010',\n",
    "    ...                              {'value': float, 'name': str, 'id': int},\n",
    "    ...                              freq='2H', partition_freq='1D', seed=1)\n",
    "    >>> df.head()  # doctest: +SKIP\n",
    "                           id      name     value\n",
    "    2000-01-01 00:00:00   969     Jerry -0.309014\n",
    "    2000-01-01 02:00:00  1010       Ray -0.760675\n",
    "    2000-01-01 04:00:00  1016  Patricia -0.063261\n",
    "    2000-01-01 06:00:00   960   Charlie  0.788245\n",
    "    2000-01-01 08:00:00  1031     Kevin  0.466002\n",
    "    \"\"\"\n",
    "    divisions = list(pd.date_range(start=start, end=end, freq=partition_freq))\n",
    "    state_data = random_state_data(len(divisions) - 1, seed)\n",
    "    name = \"make-timeseries-\" + tokenize(\n",
    "        start, end, dtypes, freq, partition_freq, state_data\n",
    "    )\n",
    "    dsk = {\n",
    "        (name, i): (\n",
    "            make_timeseries_part,\n",
    "            divisions[i],\n",
    "            divisions[i + 1],\n",
    "            dtypes,\n",
    "            freq,\n",
    "            state_data[i],\n",
    "            kwargs,\n",
    "        )\n",
    "        for i in range(len(divisions) - 1)\n",
    "    }\n",
    "    head = make_timeseries_part(\"2000\", \"2000\", dtypes, \"1H\", state_data[0], kwargs)\n",
    "    return DataFrame(dsk, name, head, divisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:00:00</th>\n",
       "      <td>952</td>\n",
       "      <td>Norbert</td>\n",
       "      <td>-0.798806</td>\n",
       "      <td>0.219038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:00:10</th>\n",
       "      <td>996</td>\n",
       "      <td>Kevin</td>\n",
       "      <td>-0.616409</td>\n",
       "      <td>0.515214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:00:20</th>\n",
       "      <td>976</td>\n",
       "      <td>Quinn</td>\n",
       "      <td>-0.372212</td>\n",
       "      <td>0.004415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:00:30</th>\n",
       "      <td>1047</td>\n",
       "      <td>Dan</td>\n",
       "      <td>0.632041</td>\n",
       "      <td>-0.688559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:00:40</th>\n",
       "      <td>1033</td>\n",
       "      <td>Ingrid</td>\n",
       "      <td>0.085113</td>\n",
       "      <td>-0.951881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-30 23:59:20</th>\n",
       "      <td>960</td>\n",
       "      <td>Alice</td>\n",
       "      <td>-0.793394</td>\n",
       "      <td>-0.378079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-30 23:59:30</th>\n",
       "      <td>1028</td>\n",
       "      <td>Zelda</td>\n",
       "      <td>-0.310456</td>\n",
       "      <td>0.477492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-30 23:59:40</th>\n",
       "      <td>1004</td>\n",
       "      <td>Michael</td>\n",
       "      <td>-0.854157</td>\n",
       "      <td>-0.041718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-30 23:59:50</th>\n",
       "      <td>1049</td>\n",
       "      <td>Ingrid</td>\n",
       "      <td>-0.137482</td>\n",
       "      <td>0.102481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-31 00:00:00</th>\n",
       "      <td>966</td>\n",
       "      <td>Hannah</td>\n",
       "      <td>-0.006672</td>\n",
       "      <td>0.020102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>259201 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id     name         x         y\n",
       "timestamp                                             \n",
       "2000-01-01 00:00:00   952  Norbert -0.798806  0.219038\n",
       "2000-01-01 00:00:10   996    Kevin -0.616409  0.515214\n",
       "2000-01-01 00:00:20   976    Quinn -0.372212  0.004415\n",
       "2000-01-01 00:00:30  1047      Dan  0.632041 -0.688559\n",
       "2000-01-01 00:00:40  1033   Ingrid  0.085113 -0.951881\n",
       "...                   ...      ...       ...       ...\n",
       "2000-01-30 23:59:20   960    Alice -0.793394 -0.378079\n",
       "2000-01-30 23:59:30  1028    Zelda -0.310456  0.477492\n",
       "2000-01-30 23:59:40  1004  Michael -0.854157 -0.041718\n",
       "2000-01-30 23:59:50  1049   Ingrid -0.137482  0.102481\n",
       "2000-01-31 00:00:00   966   Hannah -0.006672  0.020102\n",
       "\n",
       "[259201 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import utils\n",
    "\n",
    "ts = make_timeseries_part()\n",
    "ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll write it to the two formats, first CSV and then parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.3 s, sys: 68.1 ms, total: 1.37 s\n",
      "Wall time: 1.44 s\n"
     ]
    }
   ],
   "source": [
    "%time ts.to_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 139 ms, sys: 63 ms, total: 202 ms\n",
      "Wall time: 204 ms\n"
     ]
    }
   ],
   "source": [
    "%time ts.to_parquet(\"data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already we see that parquet can be faster at writing, at least for this dataset (it may be slower for small dataframes). But the difference is even more striking when you try to read that data back in.\n",
    "\n",
    "CSV is a plaintext format. This can be nice if you want to visually inspect the file. However, it's often slower and (at least for CSV) lacks any way to store data types in the file itself. Let's read them back in with the default arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 283 ms, sys: 48.2 ms, total: 331 ms\n",
      "Wall time: 335 ms\n"
     ]
    }
   ],
   "source": [
    "%time csv = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.5 ms, sys: 25.1 ms, total: 59.6 ms\n",
      "Wall time: 295 ms\n"
     ]
    }
   ],
   "source": [
    "%time parquet = pd.read_parquet(\"data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet is about 5x faster. But more importantly, the data read from CSV doesn't exactly match what was written. The original dtypes are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id         int64\n",
       "name    category\n",
       "x        float64\n",
       "y        float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV read them back in as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp     object\n",
       "id             int64\n",
       "name          object\n",
       "x            float64\n",
       "y            float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And parquet as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id         int64\n",
       "name    category\n",
       "x        float64\n",
       "y        float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When reading CSVs, pandas has to infer the dtypes. This is slow (especially for datetimes, so pandas doesn't infer datetimes by default) or impossible for more exotic types like Categorical. You'd need to store these types seperately and provide them explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 331 ms, sys: 48 ms, total: 379 ms\n",
      "Wall time: 386 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dtype = {\"name\": pd.CategoricalDtype(names)}\n",
    "csv = pd.read_csv(\n",
    "    \"data.csv\",\n",
    "    parse_dates=[\"timestamp\"],\n",
    "    dtype=dtype,\n",
    "    index_col=\"timestamp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in addition to being faster than CSVs (at least beyond small datasets), parquet can better \n",
    "preserve the data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Reading Parts of the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "When you're optitmizing some piece of code, the fastest way to do something is to not do it at all. Some readers (including both `read_csv` and `read_parquet`) have support for selecting subsets of the data for reading. Both `read_csv` and `read_parquet` let you select a subset of the columns to read in. By not having to read / parse other parts of the dataset, you speed up the reading of the parts you care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.1 ms, sys: 14.4 ms, total: 35.5 ms\n",
      "Wall time: 23 ms\n"
     ]
    }
   ],
   "source": [
    "%time _ = pd.read_parquet(\"data.parquet\", columns=[\"x\", \"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`read_parquet` also supports selecting a subset of *rows* to read. For maximum performance, you'll want to partition the dataset on disk according to your access pattern. For example, if we want to select a subset of the names, we'd partition on `name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.to_parquet(\"data-split.parquet\", partition_cols=[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.16 ms, sys: 4.28 ms, total: 12.4 ms\n",
      "Wall time: 10.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ts_frank = pd.read_parquet(\n",
    "    \"data-split.parquet\",\n",
    "    columns=[\"x\", \"y\"],\n",
    "    filters=[(\"name\", \"=\", \"Frank\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've gone from about 300ms to read the full dataset with CSV to about 14ms to read this subset with Parquet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Constructing DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "A common pattern is to store a full dataset as a bunch of files on disk with the same structure. Suppose we have a directory of parquet files that are generated by some batch process that runs at the end of the month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = list(pd.date_range(start=\"2000-01-01\", end=\"2010-12-31\", freq=\"1M\"))\n",
    "for i in range(len(months) - 1):\n",
    "    start, end = months[i], months[i + 1]\n",
    "    df = make_timeseries_part(start, end, freq=\"5T\")\n",
    "    df.to_parquet(f\"../data/{start}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000-01-31 00:00:00.parquet\n",
      "2000-02-29 00:00:00.parquet\n",
      "2000-03-31 00:00:00.parquet\n",
      "2000-04-30 00:00:00.parquet\n",
      "2000-05-31 00:00:00.parquet\n",
      "2000-06-30 00:00:00.parquet\n",
      "2000-07-31 00:00:00.parquet\n",
      "2000-08-31 00:00:00.parquet\n",
      "2000-09-30 00:00:00.parquet\n",
      "2000-10-31 00:00:00.parquet\n"
     ]
    }
   ],
   "source": [
    "ls ../data/ | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's suppose we wanted to go from disk to a single pandas dataframe. We have two ways we could get there\n",
    "\n",
    "1. Initialize one DataFrame and append to that.\n",
    "2. Make many smaller DataFrames and concatenate them together at the end.\n",
    "\n",
    "If you were using Python data structures (lists, dictionaries, sets) you'd probably use the first way. In pandas (and NumPy) the second route is faster. Let's compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "files = list(pathlib.Path(\"../data\").glob(\"*.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first method: creating an empty DataFrame and appending to it. We'll see that it's relatively slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.6 s, sys: 935 ms, total: 2.54 s\n",
      "Wall time: 2.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = pd.DataFrame({\n",
    "    \"id\": np.array([], dtype=\"int64\"),\n",
    "    \"name\": pd.Categorical([], categories=names),\n",
    "    \"x\": np.array([], dtype=\"float64\"),\n",
    "    \"y\": np.array([], dtype=\"float64\")\n",
    "}, index=pd.DatetimeIndex([], name='timestamp'))\n",
    "\n",
    "\n",
    "for file in files:\n",
    "    df_part = pd.read_parquet(file)\n",
    "    result = result.append(df_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 700 ms, sys: 220 ms, total: 920 ms\n",
      "Wall time: 722 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "parts = [pd.read_parquet(file) for file in files]\n",
    "ts_full = pd.concat(parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have something like a 2X speedup, by simply reading first and then concatenating. Why is that?\n",
    "\n",
    "pandas' `DataFrame.append` is modeled after Python's `list.append`, but memory-wise they're very differnt. Recall that the columns inside a pandas DataFrame are typically NumPy arrays, and these cannot be expanded inplace. Expanding a NumPy array really means copying the whole thing. Therefore, every append means copying the old df and then add the new df rows.\n",
    "\n",
    "So repeatedly calling DataFrame.append means repeatedly copying a whole bunch of NumPy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "Recent versions of pandas feature *nullable data types*. In addition to being more sound with the types of data, they can offer performance improvements. Because Pandas cast ints with Nans to floats and boolean with Nans to object dtypes.\n",
    "\n",
    "For example, let's generate some boolean data with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    True\n",
       "0    True\n",
       "0    True\n",
       "0    True\n",
       "0    True\n",
       "     ... \n",
       "2     NaN\n",
       "2     NaN\n",
       "2     NaN\n",
       "2     NaN\n",
       "2     NaN\n",
       "Length: 30000, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = pd.Series(\n",
    "    np.array([True, False, np.nan], dtype=object)\n",
    ").repeat(10000)\n",
    "s1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `s1.dtype` is `object`. That's because NumPy doesn't have a boolean dtype that can store missing values. So pandas falls back to an object-dtype ndarray of *Python* objects, which don't benefit from NumPy's typically optimizations. The memory usage will be higher, and operations will be slower.\n",
    "\n",
    "We can use pandas' nullable boolean dtype by calling `pd.array()` or by specifying `dtype=\"boolean\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    True\n",
       "0    True\n",
       "0    True\n",
       "0    True\n",
       "0    True\n",
       "     ... \n",
       "2    <NA>\n",
       "2    <NA>\n",
       "2    <NA>\n",
       "2    <NA>\n",
       "2    <NA>\n",
       "Length: 30000, dtype: boolean"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2 = pd.Series(\n",
    "    pd.array([True, False, pd.NA], dtype=\"boolean\")\n",
    ").repeat(10000)\n",
    "s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there are missing values, pandas' nullable boolean type takes less memory than the object dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.133333333333334"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.memory_usage(deep=True) / s2.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And operations (like reductions, comparisons, arithmetic, logical operations) take less time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.7 ms ± 1.73 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit s1 | s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.5 µs ± 2.29 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit s2 | s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently something is being cached, but ignoring that we're still much faster using pandas' nullable type.\n",
    "\n",
    "Likewise for reductions like `sum` or `mean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.07 ms ± 34.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit s1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.4 µs ± 20.5 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit s2.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spend some time ensuring that your dtypes look correct. You'll want to avoid `object` dtype whenever possible. Pandas is gradually adding new extension dtypes for more types of data, so object dtype should become rarer.\n",
    "\n",
    "It's worth mentioning pandas' `Categorical` dtype. This is a \"dictionary encoded\" type, where we store the unique set of allowed values once (`.categories`) and the specific value for a row as a compressed integer (`.codes`). This gives lower memory usage and (sometimes) faster operations.\n",
    "\n",
    "For example, `name` is a Categorical storing strings. Let's compare operations on it with an `object` dtype version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp\n",
       "2000-01-01 00:00:00    Norbert\n",
       "2000-01-01 00:00:10      Kevin\n",
       "2000-01-01 00:00:20      Quinn\n",
       "2000-01-01 00:00:30        Dan\n",
       "2000-01-01 00:00:40     Ingrid\n",
       "                        ...   \n",
       "2000-01-30 23:59:20      Alice\n",
       "2000-01-30 23:59:30      Zelda\n",
       "2000-01-30 23:59:40    Michael\n",
       "2000-01-30 23:59:50     Ingrid\n",
       "2000-01-31 00:00:00     Hannah\n",
       "Freq: 10S, Name: name, Length: 259201, dtype: category\n",
       "Categories (26, object): ['Alice', 'Bob', 'Charlie', 'Dan', ..., 'Wendy', 'Xavier', 'Yvonne', 'Zelda']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name = ts['name']\n",
    "name_obj = name.astype(object)\n",
    "display(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, the object-dtype version uses more memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.806870642799707"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_obj.memory_usage(deep=True) / name.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operations like `.value_counts()` are faster on the categorical version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.6 ms ± 1.82 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit name_obj.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.29 ms ± 163 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be warned that `Categorical` isn't purely an optimization. It does change the semantics of some operations (especially around ordering and \"unobserved\" categories)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try as much as possible to spend as much time as possible in the C code to get the best performance, i.e. use vectorized operations and dtypes other than object because object dtype is numpy array of Python objects. Therefore, any operation on them means go to Python code which is known to be slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Iteration, Apply, and Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the keys to achieving high-performance in Pandas (and Python, genenerally) is to avoid doing too much in Python. We want to push the computationally expensive pieces down to compiled languages like C.\n",
    "\n",
    "Let's suppose we have some data on airports, and wanted to compute the pairwise distances between each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iata</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00M</th>\n",
       "      <td>Thigpen</td>\n",
       "      <td>Bay Springs</td>\n",
       "      <td>MS</td>\n",
       "      <td>USA</td>\n",
       "      <td>31.953765</td>\n",
       "      <td>-89.234505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00R</th>\n",
       "      <td>Livingston Municipal</td>\n",
       "      <td>Livingston</td>\n",
       "      <td>TX</td>\n",
       "      <td>USA</td>\n",
       "      <td>30.685861</td>\n",
       "      <td>-95.017928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00V</th>\n",
       "      <td>Meadow Lake</td>\n",
       "      <td>Colorado Springs</td>\n",
       "      <td>CO</td>\n",
       "      <td>USA</td>\n",
       "      <td>38.945749</td>\n",
       "      <td>-104.569893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01G</th>\n",
       "      <td>Perry-Warsaw</td>\n",
       "      <td>Perry</td>\n",
       "      <td>NY</td>\n",
       "      <td>USA</td>\n",
       "      <td>42.741347</td>\n",
       "      <td>-78.052081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01J</th>\n",
       "      <td>Hilliard Airpark</td>\n",
       "      <td>Hilliard</td>\n",
       "      <td>FL</td>\n",
       "      <td>USA</td>\n",
       "      <td>30.688012</td>\n",
       "      <td>-81.905944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57B</th>\n",
       "      <td>Islesboro</td>\n",
       "      <td>Islesboro</td>\n",
       "      <td>ME</td>\n",
       "      <td>USA</td>\n",
       "      <td>44.302856</td>\n",
       "      <td>-68.910587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57C</th>\n",
       "      <td>East Troy Municipal</td>\n",
       "      <td>East Troy</td>\n",
       "      <td>WI</td>\n",
       "      <td>USA</td>\n",
       "      <td>42.797111</td>\n",
       "      <td>-88.372500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59B</th>\n",
       "      <td>Newton</td>\n",
       "      <td>Jackman</td>\n",
       "      <td>ME</td>\n",
       "      <td>USA</td>\n",
       "      <td>45.631991</td>\n",
       "      <td>-70.247289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5A4</th>\n",
       "      <td>Okolona Mun.-Richard M. Stovall</td>\n",
       "      <td>Okolona</td>\n",
       "      <td>MS</td>\n",
       "      <td>USA</td>\n",
       "      <td>34.015805</td>\n",
       "      <td>-88.726189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5A6</th>\n",
       "      <td>Winona-Montgomery County</td>\n",
       "      <td>Winona</td>\n",
       "      <td>MS</td>\n",
       "      <td>USA</td>\n",
       "      <td>33.465401</td>\n",
       "      <td>-89.729248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 name              city state country  \\\n",
       "iata                                                                    \n",
       "00M                           Thigpen       Bay Springs    MS     USA   \n",
       "00R              Livingston Municipal        Livingston    TX     USA   \n",
       "00V                       Meadow Lake  Colorado Springs    CO     USA   \n",
       "01G                      Perry-Warsaw             Perry    NY     USA   \n",
       "01J                  Hilliard Airpark          Hilliard    FL     USA   \n",
       "...                               ...               ...   ...     ...   \n",
       "57B                         Islesboro         Islesboro    ME     USA   \n",
       "57C               East Troy Municipal         East Troy    WI     USA   \n",
       "59B                            Newton           Jackman    ME     USA   \n",
       "5A4   Okolona Mun.-Richard M. Stovall           Okolona    MS     USA   \n",
       "5A6          Winona-Montgomery County            Winona    MS     USA   \n",
       "\n",
       "       latitude   longitude  \n",
       "iata                         \n",
       "00M   31.953765  -89.234505  \n",
       "00R   30.685861  -95.017928  \n",
       "00V   38.945749 -104.569893  \n",
       "01G   42.741347  -78.052081  \n",
       "01J   30.688012  -81.905944  \n",
       "...         ...         ...  \n",
       "57B   44.302856  -68.910587  \n",
       "57C   42.797111  -88.372500  \n",
       "59B   45.631991  -70.247289  \n",
       "5A4   34.015805  -88.726189  \n",
       "5A6   33.465401  -89.729248  \n",
       "\n",
       "[500 rows x 6 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airports = pd.read_csv(\n",
    "    \"https://vega.github.io/vega-datasets/data/airports.csv\",\n",
    "    index_col=\"iata\",\n",
    "    nrows=500,\n",
    ")\n",
    "airports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do a bit of renaming and reindexing to generate the DataFrame of pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>longitude_orig</th>\n",
       "      <th>latitude_orig</th>\n",
       "      <th>longitude_dest</th>\n",
       "      <th>latitude_dest</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orig</th>\n",
       "      <th>dest</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">00M</th>\n",
       "      <th>00M</th>\n",
       "      <td>-89.234505</td>\n",
       "      <td>31.953765</td>\n",
       "      <td>-89.234505</td>\n",
       "      <td>31.953765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00R</th>\n",
       "      <td>-89.234505</td>\n",
       "      <td>31.953765</td>\n",
       "      <td>-95.017928</td>\n",
       "      <td>30.685861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00V</th>\n",
       "      <td>-89.234505</td>\n",
       "      <td>31.953765</td>\n",
       "      <td>-104.569893</td>\n",
       "      <td>38.945749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01G</th>\n",
       "      <td>-89.234505</td>\n",
       "      <td>31.953765</td>\n",
       "      <td>-78.052081</td>\n",
       "      <td>42.741347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01J</th>\n",
       "      <td>-89.234505</td>\n",
       "      <td>31.953765</td>\n",
       "      <td>-81.905944</td>\n",
       "      <td>30.688012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">5A6</th>\n",
       "      <th>57B</th>\n",
       "      <td>-89.729248</td>\n",
       "      <td>33.465401</td>\n",
       "      <td>-68.910587</td>\n",
       "      <td>44.302856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57C</th>\n",
       "      <td>-89.729248</td>\n",
       "      <td>33.465401</td>\n",
       "      <td>-88.372500</td>\n",
       "      <td>42.797111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59B</th>\n",
       "      <td>-89.729248</td>\n",
       "      <td>33.465401</td>\n",
       "      <td>-70.247289</td>\n",
       "      <td>45.631991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5A4</th>\n",
       "      <td>-89.729248</td>\n",
       "      <td>33.465401</td>\n",
       "      <td>-88.726189</td>\n",
       "      <td>34.015805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5A6</th>\n",
       "      <td>-89.729248</td>\n",
       "      <td>33.465401</td>\n",
       "      <td>-89.729248</td>\n",
       "      <td>33.465401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           longitude_orig  latitude_orig  longitude_dest  latitude_dest\n",
       "orig dest                                                              \n",
       "00M  00M       -89.234505      31.953765      -89.234505      31.953765\n",
       "     00R       -89.234505      31.953765      -95.017928      30.685861\n",
       "     00V       -89.234505      31.953765     -104.569893      38.945749\n",
       "     01G       -89.234505      31.953765      -78.052081      42.741347\n",
       "     01J       -89.234505      31.953765      -81.905944      30.688012\n",
       "...                   ...            ...             ...            ...\n",
       "5A6  57B       -89.729248      33.465401      -68.910587      44.302856\n",
       "     57C       -89.729248      33.465401      -88.372500      42.797111\n",
       "     59B       -89.729248      33.465401      -70.247289      45.631991\n",
       "     5A4       -89.729248      33.465401      -88.726189      34.015805\n",
       "     5A6       -89.729248      33.465401      -89.729248      33.465401\n",
       "\n",
       "[250000 rows x 4 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"longitude\", \"latitude\"]\n",
    "idx = pd.MultiIndex.from_product([airports.index, airports.index],\n",
    "                                 names=['orig', 'dest'])\n",
    "\n",
    "pairs = pd.concat([\n",
    "    airports[columns]\n",
    "        .add_suffix('_orig')\n",
    "        .reindex(idx, level='orig'),\n",
    "    airports[columns]\n",
    "        .add_suffix('_dest')\n",
    "        .reindex(idx, level='dest')\n",
    "    ], axis=\"columns\"\n",
    ")\n",
    "pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's consider two implementations of the great circle distance computation.\n",
    "\n",
    "The first will use pure Python, and computes the distance between two points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def gcd_py(lat1, lng1, lat2, lng2):\n",
    "    '''\n",
    "    Calculate great circle distance between two points.\n",
    "    https://www.johndcook.com/blog/python_longitude_latitude/\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lat1, lng1, lat2, lng2: float\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    distance:\n",
    "      distance from ``(lat1, lng1)`` to ``(lat2, lng2)`` in kilometers.\n",
    "    '''\n",
    "    # python2 users will have to use ascii identifiers (or upgrade)\n",
    "    degrees_to_radians = math.pi / 180.0\n",
    "    ϕ1 = (90 - lat1) * degrees_to_radians\n",
    "    ϕ2 = (90 - lat2) * degrees_to_radians\n",
    "\n",
    "    θ1 = lng1 * degrees_to_radians\n",
    "    θ2 = lng2 * degrees_to_radians\n",
    "\n",
    "    cos = (math.sin(ϕ1) * math.sin(ϕ2) * math.cos(θ1 - θ2) +\n",
    "           math.cos(ϕ1) * math.cos(ϕ2))\n",
    "    # round to avoid precision issues on identical points causing ValueErrors\n",
    "    cos = round(cos, 8)\n",
    "    arc = math.acos(cos)\n",
    "    return arc * 6373  # radius of earth, in kilometers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second uses NumPy, and computes the distances between *arrays* of points.\n",
    "Notice how similar the two implementations are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcd_vec(lat1, lng1, lat2, lng2):\n",
    "    '''\n",
    "    Calculate great circle distance.\n",
    "    https://www.johndcook.com/blog/python_longitude_latitude/\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lat1, lng1, lat2, lng2: float or array of float\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    distance:\n",
    "      distance from ``(lat1, lng1)`` to ``(lat2, lng2)`` in kilometers.\n",
    "    '''\n",
    "    ϕ1 = np.deg2rad(90 - lat1)\n",
    "    ϕ2 = np.deg2rad(90 - lat2)\n",
    "\n",
    "    θ1 = np.deg2rad(lng1)\n",
    "    θ2 = np.deg2rad(lng2)\n",
    "\n",
    "    cos = (np.sin(ϕ1) * np.sin(ϕ2) * np.cos(θ1 - θ2) +\n",
    "           np.cos(ϕ1) * np.cos(ϕ2))\n",
    "    # round to avoid precision issues on identical points causing warnings\n",
    "    cos = np.round(cos, 8)\n",
    "    arc = np.arccos(cos)\n",
    "    return arc * 6373 # radius of earth, in kilometers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's use these functions in a few different ways.\n",
    "\n",
    "1. Pass `gcd_py` to `DataFrame.apply`\n",
    "2. Manually iterate over the DataFrame, calling `gcd_py` on each row\n",
    "3. Call `gcd_vec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.83 s, sys: 57.2 ms, total: 4.89 s\n",
      "Wall time: 5.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# gcd_py with DataFrame.apply\n",
    "r = pairs.apply(\n",
    "    lambda x: gcd_py(x['latitude_orig'],\n",
    "                     x['longitude_orig'],\n",
    "                     x['latitude_dest'],\n",
    "                     x['longitude_dest']),\n",
    "                axis=\"columns\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 589 ms, sys: 17.1 ms, total: 607 ms\n",
      "Wall time: 608 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# gcd_py with manual iteration\n",
    "_ = pd.Series([gcd_py(*x) for x in pairs.itertuples(index=False)],\n",
    "              index=pairs.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35 ms, sys: 7.79 ms, total: 42.8 ms\n",
      "Wall time: 49.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# gcd_vec\n",
    "r = gcd_vec(pairs['latitude_orig'], pairs['longitude_orig'],\n",
    "            pairs['latitude_dest'], pairs['longitude_dest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orig  dest\n",
       "00M   00M        0.000000\n",
       "      00R      567.271820\n",
       "      00V     1589.259385\n",
       "      01G     1551.898663\n",
       "      01J      710.296324\n",
       "dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance-wise, it's clear that the vectorized version is best. And, in my opinion, the code is clearer.\n",
    "\n",
    "DataFrame.apply is the clear loser hear. It can be useful for quickly writing some little transformation. But `DataFrame.apply(..., axis=1)` generally should be avoided, especially for performance-sensitive code. It does much more work than the other forms we showed.\n",
    "\n",
    "Not every problem can be solved with vecorization though. Some problems are difficult or impossible to express using just Numpy. For those, we fortunately have Numba. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Using Numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "Recent versions of pandas optionally make extensive use of Numba to speed up certain operations. This is helpful when you have some custom user-defined function that you're passing to one of pandas' `.apply`, `.agg`, or `.transform` methods (in a rolling or groupby context).\n",
    "\n",
    "Consider something like a `df.rolling(n).apply(func)`. At a high level, that operation requires\n",
    "\n",
    "1. Splitting the input into groups\n",
    "2. Applying `func` to each group\n",
    "3. Collecting the results into an output group\n",
    "\n",
    "\n",
    "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vSpZlYnXg8MfRHlRjm8JDcxkCjrQfI2XoS06JikaoRCuZiQUUgyo5yjWASU-ynNcucK2-eumooIty1-/pub?w=960&amp;h=720\">\n",
    "\n",
    "Now let's suppose we wanted to speed that up with Numba. As a user, you could `@numba.jit` your function. Depending on what your user defined function is doing, that could lead to a nice speedup. But there would still be a bunch of overhead *around* your function that would be relatively slow. Pandas would need to slice into the array (from Python), call your fast function (now in fast machine code), and jump back to Python to form the output array.\n",
    "\n",
    "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vRwvBtrV51LU2qfOxXUrggJ7h0-bTeSSozatQ7AECyhSOxEdO0ivfoXNhwWM5Q-lZvRBxmPMeAX5hzf/pub?w=960&amp;h=540\">\n",
    "\n",
    "When you use the `engine=\"numba\"` keyword, pandas and Numba able to JIT compile a lot more than just your function. We're able to JIT the entire splitting, function application, and result combination so that the whole things stays in fast machine code.\n",
    "\n",
    "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vRYpI3MI4LKZQSz2VUAxQrxiN6wAlnmTCLOF2VcYTDtF5dJEbSE6IY1MgFH8w8GH84Q2Suu9ngjgYD0/pub?w=960&amp;h=540\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's compute the mean absolute deviation. Pandas doesn't have a builtin version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mad(x):\n",
    "    return np.fabs(x - x.mean()).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset `ts` is 10-second frequency. We'll do a rolling mean absolute deviation at 1-minute frequency. But, the naive version is too slow to do on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 324 ms, sys: 6.46 ms, total: 331 ms\n",
      "Wall time: 331 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:00:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:00:10</th>\n",
       "      <td>0.091198</td>\n",
       "      <td>0.148088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:00:20</th>\n",
       "      <td>0.149065</td>\n",
       "      <td>0.179328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:00:30</th>\n",
       "      <td>0.460444</td>\n",
       "      <td>0.354599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:00:40</th>\n",
       "      <td>0.458105</td>\n",
       "      <td>0.511892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-02 03:45:50</th>\n",
       "      <td>0.317898</td>\n",
       "      <td>0.558219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-02 03:46:00</th>\n",
       "      <td>0.452094</td>\n",
       "      <td>0.574206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-02 03:46:10</th>\n",
       "      <td>0.456849</td>\n",
       "      <td>0.547890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-02 03:46:20</th>\n",
       "      <td>0.470734</td>\n",
       "      <td>0.569063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-02 03:46:30</th>\n",
       "      <td>0.472504</td>\n",
       "      <td>0.417875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            x         y\n",
       "timestamp                              \n",
       "2000-01-01 00:00:00  0.000000  0.000000\n",
       "2000-01-01 00:00:10  0.091198  0.148088\n",
       "2000-01-01 00:00:20  0.149065  0.179328\n",
       "2000-01-01 00:00:30  0.460444  0.354599\n",
       "2000-01-01 00:00:40  0.458105  0.511892\n",
       "...                       ...       ...\n",
       "2000-01-02 03:45:50  0.317898  0.558219\n",
       "2000-01-02 03:46:00  0.452094  0.574206\n",
       "2000-01-02 03:46:10  0.456849  0.547890\n",
       "2000-01-02 03:46:20  0.470734  0.569063\n",
       "2000-01-02 03:46:30  0.472504  0.417875\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# For speed, limit to 10,000 rows\n",
    "ts[[\"x\", \"y\"]].head(10_000).rolling(\"T\").apply(mad, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try this with `engine=\"numba\"`. At first, things don't look great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.12 s, sys: 152 ms, total: 1.27 s\n",
      "Wall time: 3.16 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:00:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:00:10</th>\n",
       "      <td>0.091198</td>\n",
       "      <td>0.148088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:00:20</th>\n",
       "      <td>0.149065</td>\n",
       "      <td>0.179328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:00:30</th>\n",
       "      <td>0.460444</td>\n",
       "      <td>0.354599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:00:40</th>\n",
       "      <td>0.458105</td>\n",
       "      <td>0.511892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-02 03:45:50</th>\n",
       "      <td>0.317898</td>\n",
       "      <td>0.558219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-02 03:46:00</th>\n",
       "      <td>0.452094</td>\n",
       "      <td>0.574206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-02 03:46:10</th>\n",
       "      <td>0.456849</td>\n",
       "      <td>0.547890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-02 03:46:20</th>\n",
       "      <td>0.470734</td>\n",
       "      <td>0.569063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-02 03:46:30</th>\n",
       "      <td>0.472504</td>\n",
       "      <td>0.417875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            x         y\n",
       "timestamp                              \n",
       "2000-01-01 00:00:00  0.000000  0.000000\n",
       "2000-01-01 00:00:10  0.091198  0.148088\n",
       "2000-01-01 00:00:20  0.149065  0.179328\n",
       "2000-01-01 00:00:30  0.460444  0.354599\n",
       "2000-01-01 00:00:40  0.458105  0.511892\n",
       "...                       ...       ...\n",
       "2000-01-02 03:45:50  0.317898  0.558219\n",
       "2000-01-02 03:46:00  0.452094  0.574206\n",
       "2000-01-02 03:46:10  0.456849  0.547890\n",
       "2000-01-02 03:46:20  0.470734  0.569063\n",
       "2000-01-02 03:46:30  0.472504  0.417875\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ts[[\"x\", \"y\"]].head(10_000).rolling(\"T\").apply(\n",
    "    mad, engine=\"numba\", raw=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the operation is a bit slower. But that's becuase Numba is a just-in-time compiler. It observes what your code is doing and compiles some machine code tailored to the work being done. That compilation takes time, so it's cached and reused.  We can call it again and see that things are even faster on subsequent calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.69 ms ± 125 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ts[[\"x\", \"y\"]].head(10_000).rolling(\"T\").apply(mad, engine=\"numba\", raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, it's fast enough that we can call it on the whole thing now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 ms ± 5.99 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = ts[[\"x\", \"y\"]].rolling(\"T\").apply(\n",
    "    mad, engine=\"numba\", raw=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using numba for user-defined functions passed to pandas' apply, agg, and transform is extremely powerful. In the currently released version of pandas (1.1) numba-accelerated operations are available in\n",
    "\n",
    "* GroupBy.aggregate\n",
    "* GroupBy.transform\n",
    "* Rolling/Expanding.apply\n",
    "* Rolling/Expanding.aggregate\n",
    "\n",
    "## Summary\n",
    "\n",
    "Today we've seen a few strategies for writing high-performance pandas code\n",
    "\n",
    "1. Choose the best file format for your needs\n",
    "\n",
    "File formats like Parquet can offer higher performance, especially if your workload only needs to read in subsets of the data\n",
    "\n",
    "2. Avoid reapeatedly expanding DataFrames along the rows\n",
    "\n",
    "We saw that repeatedly calling DataFrame.append was slower than building many dataframes and concatentating them at the end.\n",
    "\n",
    "3. Use the right data type\n",
    "\n",
    "We saw that using pandas' new nullable types can avoid slow `object`-dtypes and cut down on memory usage.\n",
    "\n",
    "4. Avoid iteration and apply\n",
    "\n",
    "We implemented two versions of the great circle distance computation. In pandas, the vectorized version using NumPy was faster than the NumPy version.\n",
    "\n",
    "5. Use Numba for user-defined functions\n",
    "\n",
    "Pandas may not always have a built-in version of the method you need."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
